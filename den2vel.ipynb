{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 678,
   "id": "b0e6ea84",
   "metadata": {
    "cellId": "jftzi84moejotkzs217x0b"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from pathlib import Path\n",
    "no_obs_path = Path('/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "0cf42cbf",
   "metadata": {
    "cellId": "f3kajhkxwn9s9s409e17fh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "id": "4a770731",
   "metadata": {
    "cellId": "jolsyg0d1vqo3flh3zt26g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Union, Callable, Optional, List\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "id": "980a585a",
   "metadata": {
    "cellId": "pv4ww5g24mte6tsi07969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:00, 2099.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "val_sims = ['/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v01/sim_1001',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v01/sim_1030',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v01/sim_1058',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v02/sim_1001',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v02/sim_1030',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v02/sim_1058',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v03/sim_1001',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v03/sim_1030',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v03/sim_1058',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v04/sim_1001',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v04/sim_1030',\n",
    "            '/home/jupyter/mnt/datasets/den2vel/datasets/2D_no_obs/v04/sim_1058']\n",
    "\n",
    "train_sims = []\n",
    "\n",
    "for p in tqdm(os.walk(no_obs_path)):\n",
    "    if 'sim' in p[0]:\n",
    "        if p[0] not in val_sims:\n",
    "            train_sims.append(p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "61cad717",
   "metadata": {
    "cellId": "fvovf9yvlpmdmi584f4frn"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class NoObsDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, sims_pth: List[Union[str, Path]],transforms: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "        self._transforms = transforms \n",
    "        self.sims_pth = sims_pth\n",
    "        self.density = []\n",
    "        self.velocity = []\n",
    "        self.s_dict = {}\n",
    "        \n",
    "        for s in tqdm(sims_pth):\n",
    "            p = next(os.walk(s))\n",
    "            for f in p[-1]:\n",
    "                if 'npz' == f[-3:]:\n",
    "                    if 'density' in f:\n",
    "                        self.density.append(f'{p[0]}/{f}')\n",
    "                    else:\n",
    "                        self.velocity.append(f'{p[0]}/{f}')\n",
    "                elif 'json' in f:\n",
    "                    with open(f'{p[0]}/{f}', 'r') as f:\n",
    "                        loaded = json.load(f)\n",
    "                        self.s_dict[p[0]] = np.array([float(loaded['bnds']), \n",
    "                                                      float(loaded['buoyFac'])], dtype=np.float32)\n",
    "                            \n",
    "                            \n",
    "        assert len(self.density) == len(self.velocity)\n",
    "        \n",
    "        self.density.sort()\n",
    "        self.velocity.sort()\n",
    "                        \n",
    "    @property\n",
    "    def transforms(self):\n",
    "        return self._transforms\n",
    "    \n",
    "    @transforms.setter\n",
    "    def transforms(self, transforms: Callable):\n",
    "        self._transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.density)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        den_pth = self.density[index]\n",
    "        den = np.ascontiguousarray(np.load(den_pth)['arr_0'][0, ::-1])\n",
    "        vel = np.ascontiguousarray(np.load(self.velocity[index])['arr_0'][0, ::-1, :, :-1])\n",
    "        i = den_pth.find('v0')\n",
    "        s_pth = den_pth[:i + 12]\n",
    "        s = self.s_dict[s_pth]\n",
    "        \n",
    "        if self._transforms is not None:\n",
    "            den = self._transforms(den)\n",
    "            vel = self._transforms(vel)\n",
    "        return den, vel, torch.from_numpy(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "6e84170c",
   "metadata": {
    "cellId": "u4a24fsu493fsw6fcoff7"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import torchvision\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "id": "d3373e01",
   "metadata": {
    "cellId": "himv0rpz186ab8mfcx8o5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 228/228 [00:00<00:00, 1273.49it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 1100.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "no_obs_train_dataset = NoObsDataset(train_sims, transforms=transform)\n",
    "no_obs_val_dataset = NoObsDataset(val_sims, transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "64b6664b",
   "metadata": {
    "cellId": "igdapsdpqnea74g28h9g"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "BATCH_SIZE = 2\n",
    "train_loader = torch.utils.data.DataLoader(no_obs_train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = torch.utils.data.DataLoader(no_obs_val_dataset, batch_size=1, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "id": "8d9e77f1",
   "metadata": {
    "cellId": "qfompltsllcgconntu32h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_c: int, out_c: int):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Sequential(nn.Conv2d(in_c, out_c, 3, padding=1, padding_mode='reflect'),\n",
    "                                    nn.InstanceNorm2d(out_c),\n",
    "                                    nn.ReLU())\n",
    "        self.conv_2 = nn.Sequential(nn.Conv2d(out_c, out_c, 3, padding=1, padding_mode='reflect'),\n",
    "                                    nn.InstanceNorm2d(out_c))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.conv_1(x)\n",
    "        y = self.conv_2(y)\n",
    "        return x + y\n",
    "    \n",
    "    \n",
    "class SubNetS(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_c: int):\n",
    "        super().__init__()\n",
    "#         self.conv_in = nn.Sequential(nn.Conv2d(in_c, 16, 7),\n",
    "#                                     nn.ReLU())\n",
    "        self.pool = nn.AvgPool2d(8, 8)\n",
    "        self.flatten_conv = nn.Sequential(nn.Conv2d(16, 16, 3, padding=1, padding_mode='reflect'), \n",
    "                                          nn.AvgPool2d(2, 2))\n",
    "        self.s_out = nn.Sequential(nn.Flatten(), nn.Linear(16 * 4 * 4, 2))\n",
    "        self.s_in = nn.Linear(4, 4 * 8 * 8)\n",
    "        self.conv_out = nn.Sequential(nn.Conv2d(4, 8, 3, padding=1, padding_mode='reflect'),\n",
    "                                      nn.LeakyReLU(0.2),\n",
    "                                      nn.Conv2d(8, 16, 3, padding=1, padding_mode='reflect'),\n",
    "                                      nn.LeakyReLU(0.2))\n",
    "        self.unpool = nn.Sequential(nn.Upsample((64, 64), mode='bilinear', align_corners=False), \n",
    "                                    nn.Conv2d(16, 16, 1))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, s: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        #x = self.conv_in(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten_conv(x)\n",
    "        s_out = self.s_out(x)\n",
    "        if s is None:\n",
    "            s = -torch.ones_like(s_out)\n",
    "            s.requires_grad = False\n",
    "        x = torch.cat([s_out, s], dim=1)\n",
    "        x = self.s_in(x)\n",
    "        x = x.reshape(-1, 4, 8, 8)\n",
    "        x = self.conv_out(x)\n",
    "        x = self.unpool(x)\n",
    "        return x, s_out\n",
    "            \n",
    "        \n",
    "class SubNetKE(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_c: int):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Sequential(nn.Conv2d(in_c, 8, 3, padding=1, padding_mode='reflect'),\n",
    "                                    nn.AvgPool2d(2, 2), \n",
    "                                    nn.InstanceNorm2d(8),\n",
    "                                    nn.ReLU())\n",
    "        self.conv_2 = nn.Sequential(nn.Conv2d(8, 4, 3, padding=1, padding_mode='reflect'),\n",
    "                                    nn.AvgPool2d(2, 2),\n",
    "                                    nn.InstanceNorm2d(4))\n",
    "        self.ke_out_conv = nn.Conv2d(4, 1, 1)\n",
    "        self.ke_in_conv = nn.Conv2d(2, 2, 3, padding=1, padding_mode='reflect')\n",
    "        self.conv_5 = nn.Conv2d(2, 2, 3, padding=1, padding_mode='reflect')\n",
    "        self.unpool = nn.Sequential(nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False), nn.Conv2d(2, 1, 1))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, ke: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        ke_out = self.ke_out_conv(x)\n",
    "        if ke is None:\n",
    "            ke = -torch.ones_like(ke_out)\n",
    "            ke.requires_grad = False\n",
    "        x = torch.cat([ke_out, ke], dim=1) if x.ndim == 4 else torch.cat([ke_out, ke], dim=0)\n",
    "        x = self.ke_in_conv(x)\n",
    "        x = self.conv_5(x)\n",
    "        return self.unpool(x), ke_out\n",
    "        \n",
    "        \n",
    "class SubNetW(nn.Module):\n",
    "    def __init__(self, in_c: int):\n",
    "        super().__init__()\n",
    "        self.up1 = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False), \n",
    "                                 nn.Conv2d(in_c, 32, 3, padding=1, padding_mode='reflect'),\n",
    "                                 nn.InstanceNorm2d(32),\n",
    "                                 nn.ReLU())\n",
    "        self.up2 = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False), \n",
    "                                 nn.Conv2d(32, 16, 3, padding=1, padding_mode='reflect'),\n",
    "                                 nn.InstanceNorm2d(16),\n",
    "                                 nn.ReLU())\n",
    "        self.w_out = nn.Conv2d(16, 1, 7, padding='same')\n",
    "        self.w_in = nn.Sequential(nn.Conv2d(2, 16, 5, padding=1, padding_mode='reflect'),\n",
    "                                  nn.AvgPool2d(2, 2),\n",
    "                                  nn.InstanceNorm2d(16),\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.down = nn.Sequential(nn.Conv2d(16, 24, 5, padding=1, padding_mode='reflect'),\n",
    "                                  nn.AvgPool2d(2, 2),\n",
    "                                  nn.InstanceNorm2d(24),\n",
    "                                  nn.ReLU())\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(24, 32, 3, padding=1, padding_mode='reflect')\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, w: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        w_out = self.w_out(x)\n",
    "        if w is None:\n",
    "            w = -torch.ones_like(w_out)\n",
    "            w.requires_grad = False\n",
    "        x = torch.cat([w_out, w], dim=1) if x.ndim == 4 else torch.cat([w_out, w], dim=0)\n",
    "        x = self.w_in(x)\n",
    "        x = self.down(x)\n",
    "        x = self.conv_out(x)\n",
    "        return x, w_out\n",
    "    \n",
    "    \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_c: int, out_c: int, s_subnet: nn.Module, \n",
    "                 w_subnet: nn.Module, ke_subnet: nn.Module, obstacles: bool = False):\n",
    "        super().__init__()\n",
    "        self.obstacles = obstacles\n",
    "        self.down_1 = nn.Sequential(nn.Conv2d(in_c + obstacles, 16, 3, padding='same', padding_mode='reflect'),\n",
    "                                    nn.InstanceNorm2d(16),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(16, 32, 3, padding='same'),\n",
    "                                    nn.InstanceNorm2d(32),\n",
    "                                    nn.ReLU())\n",
    "        self.down_2 = nn.Sequential(nn.Conv2d(32, 64, 3, padding='same', padding_mode='reflect'),\n",
    "                                    nn.AvgPool2d(2, 2),\n",
    "                                    nn.InstanceNorm2d(64),\n",
    "                                    nn.ReLU())\n",
    "        \n",
    "        self.down_3 = nn.Sequential(nn.Conv2d(64, 128, 3, padding='same', padding_mode='reflect'),\n",
    "                                    nn.AvgPool2d(2, 2),\n",
    "                                    nn.InstanceNorm2d(128),\n",
    "                                    nn.ReLU())\n",
    "        self.resblock_1 = ResBlock(128, 128)\n",
    "        self.resblock_2 = ResBlock(128, 128)\n",
    "        self.resblock_3 = ResBlock(128, 128)\n",
    "        self.resblock_4 = ResBlock(145, 145)\n",
    "        self.resblock_5 = ResBlock(145, 145)\n",
    "        self.resblock_6 = ResBlock(145, 145)\n",
    "        \n",
    "        self.subnet_conv = nn.Sequential(nn.Conv2d(128, 16, 3, padding='same', padding_mode='reflect'), \n",
    "                                         nn.ReLU())\n",
    "        \n",
    "        self.up_1 = nn.Sequential(nn.Conv2d(177, 128, 3, padding='same', padding_mode='reflect'),\n",
    "                                  nn.InstanceNorm2d(128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Conv2d(128, 128, 3, padding='same', padding_mode='reflect'),\n",
    "                                  nn.InstanceNorm2d(128),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Upsample((128, 128), mode='bilinear', align_corners=False))\n",
    "        self.up_2 = nn.Sequential(nn.Conv2d(128, 64, 3, padding='same', padding_mode='reflect'),\n",
    "                                  nn.InstanceNorm2d(64),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Upsample((256, 256), mode='bilinear', align_corners=False))\n",
    "            \n",
    "        self.up_3 = nn.Sequential(nn.Conv2d(64, 32, 3, padding='same', padding_mode='reflect'),\n",
    "                                  nn.ReLU())\n",
    "                        \n",
    "        self.conv_out = nn.Conv2d(32, out_c, 3, padding='same', padding_mode='reflect')\n",
    "        \n",
    "        self.s_subnet = s_subnet\n",
    "        self.ke_subnet = ke_subnet\n",
    "        self.w_subnet = w_subnet\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, s: Optional[torch.Tensor] = None, \n",
    "                w: Optional[torch.Tensor] = None, ke: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, \n",
    "                                                                                              torch.Tensor, torch.Tensor]:\n",
    "        x = self.down_1(x)\n",
    "        x = self.down_2(x)\n",
    "        x = self.down_3(x)\n",
    "        \n",
    "        x = self.resblock_1(x) \n",
    "        x = self.resblock_2(x)\n",
    "        x = self.resblock_3(x)\n",
    "        \n",
    "        y = self.subnet_conv(x)\n",
    "        \n",
    "        \n",
    "        y_s, s_out = self.s_subnet(y, s) # 16 x 8 x 8\n",
    "        y_ke, ke_out = self.ke_subnet(y, ke) # 1 x 16 x 16\n",
    "        \n",
    "        x = torch.cat([F.interpolate(y_s, size=(64, 64), mode='bilinear', align_corners=False), \n",
    "                       F.interpolate(y_ke, size=(64, 64), mode='bilinear', align_corners=False), x], dim=1)\n",
    "        \n",
    "        x = self.resblock_4(x)\n",
    "        x = self.resblock_5(x)\n",
    "        x = self.resblock_6(x)\n",
    "\n",
    "        y, w_out = self.w_subnet(x, w)\n",
    "        \n",
    "        \n",
    "        x = torch.cat([F.interpolate(y, size=(64, 64), mode='bilinear', align_corners=False), x], dim=1)\n",
    "        \n",
    "        x = self.up_1(x)\n",
    "        x = self.up_2(x)\n",
    "        x = self.up_3(x)\n",
    "        \n",
    "        x = self.conv_out(x)\n",
    "        \n",
    "        return x, s_out, w_out, ke_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "73f604a9",
   "metadata": {
    "cellId": "arshjhcamnpp3c9xdz4m2s"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from typing import Sequence\n",
    "def flatten(*tensors: Sequence[torch.Tensor], BATCH_SIZE: int) -> torch.Tensor:\n",
    "    return torch.cat([t.reshape(BATCH_SIZE, -1) for t in tensors], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "de364503",
   "metadata": {
    "cellId": "dritc1dltthlp0ziaxxmts"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def curl(x):\n",
    "    dy = x[:, :, 1:] - x[:, :, :-1]\n",
    "    dx = -x[..., 1:] + x[..., :-1]\n",
    "    dy = torch.cat([dy, dy[:, :, -1].unsqueeze(2)], dim=2)\n",
    "    dx = torch.cat([dx, dx[..., -1].unsqueeze(3)], dim=3)\n",
    "    return torch.cat([dy,dx], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "f1d48417",
   "metadata": {
    "cellId": "g9ms43whwusujesvauml2q"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "from typing import Mapping\n",
    "\n",
    "def train_step(G: nn.Module, D: nn.Module, d: torch.Tensor, u: torch.Tensor, \n",
    "               s: torch.Tensor, w: torch.Tensor, ke: torch.Tensor, optG: Callable, optD: Callable,\n",
    "               BATCH_SIZE: int, l_adv: float = 0.2, l_l1: float = 1.0) -> Mapping[str, torch.Tensor]:\n",
    "    opt_inputs = []\n",
    "    optG.zero_grad(True)\n",
    "    g_out, g_s, g_w, g_ke = G(d, *opt_inputs)\n",
    "    \n",
    "    g_u = curl(g_out)\n",
    "    \n",
    "    optD.zero_grad(True)\n",
    "    \n",
    "    dt_d, dt_s, dt_w, dt_ke = D(u)\n",
    "    df_d, df_s, df_w, df_ke = D(g_u.detach(), g_s.detach(), g_w.detach(), g_ke.detach())\n",
    "    \n",
    "    s_r, w_r, ke_r = s.reshape(BATCH_SIZE, -1), w.reshape(BATCH_SIZE, - 1), ke.reshape(BATCH_SIZE, -1)\n",
    "    \n",
    "    flatten_den = flatten(d, s_r, w_r, ke_r, BATCH_SIZE=BATCH_SIZE)\n",
    "    \n",
    "    flatten_dt_out = flatten(dt_d, dt_s, dt_w, dt_ke, BATCH_SIZE=BATCH_SIZE)\n",
    "    flatten_df_out = flatten(df_d, df_s, df_w, df_ke, BATCH_SIZE=BATCH_SIZE)\n",
    "    \n",
    "    D_p_loss = F.mse_loss(flatten_dt_out, flatten_den)\n",
    "    D_n_loss = F.mse_loss(flatten_df_out, flatten_den)\n",
    "    \n",
    "    #D_p_poss = F.mse_loss(dt_d, d) + F.mse_loss(dt_s, s) + F.mse_loss(dt_w, w) +  F.mse_loss(dt_ke, ke)\n",
    "    #D_n_loss = F.mse_loss(df_f, d) + F.mse_loss(df_s, s) + F.mse_loss(df_w, w) + F.mse_loss(df_ke, ke)\n",
    "    \n",
    "    k = 0.5\n",
    "    D_loss = D_p_loss - k * D_n_loss\n",
    "    D_loss.backward()\n",
    "    optD.step()\n",
    "    \n",
    "    flatten_gd_out = flatten(*D(g_u, g_s, g_w, g_ke), BATCH_SIZE=BATCH_SIZE)\n",
    "    flatten_vel = flatten(u, s_r, w_r, ke_r, BATCH_SIZE=BATCH_SIZE)\n",
    "    \n",
    "    g_out_l1, s_l1, w_l1, ke_l1 = G(d)\n",
    "    l1_out = flatten(curl(g_out_l1), s_l1, w_l1, ke_l1, BATCH_SIZE=BATCH_SIZE)\n",
    "    \n",
    "    G_loss = l_adv * F.mse_loss(flatten_gd_out, flatten_den) + l_l1 * F.l1_loss(l1_out, flatten_vel)\n",
    "    \n",
    "    G_loss.backward()\n",
    "    optG.step()\n",
    "    \n",
    "    return {'G_loss': G_loss.detach().cpu().item(),\n",
    "            'D_p_loss': D_p_loss.detach().cpu().item(),\n",
    "            'D_n_loss': D_n_loss.detach().cpu().item()}\n",
    "    \n",
    "    #k = (1 - alpha_k) * k + alpha * D_p_loss.detach() / D_n_loss.detach()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "bba8a9ac",
   "metadata": {
    "cellId": "vc8hython7ojj79e825n2h"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def energy(vel: torch.Tensor) -> torch.Tensor:\n",
    "    return F.avg_pool2d(vel.pow(2).sum(1, keepdim=True), 16, 16)\n",
    "\n",
    "def augment_energy(ke, den):\n",
    "    std = ke.std((-2, -1), keepdim=True)\n",
    "    return ke + F.avg_pool2d((torch.randn_like(den) / max(10, std.max())), 16, 16)\n",
    "\n",
    "def augment_velocity(vel):\n",
    "    eps = torch.randn_like(vel) / max(10, vel.std())\n",
    "    return vel + F.interpolate(F.avg_pool2d(eps, 16, 16), size=(256, 256), mode='bilinear', align_corners=True)\n",
    "    \n",
    "\n",
    "def train(G, D, optG, optD, train_loader, val_loader, num_epochs, device, BATCH_SIZE):\n",
    "    losses = {'G_loss': [],\n",
    "              'D_p_loss': [],\n",
    "              'D_n_loss': []}\n",
    "    for epoch in range(num_epochs):\n",
    "        for k in losses.keys():\n",
    "            losses[k].append([])\n",
    "        G.train(), D.train()\n",
    "        for den, vel, s in tqdm(train_loader):\n",
    "            den, vel, s = den.to(device), vel.to(device), s.to(device),\n",
    "            ke = energy(vel)\n",
    "#             if np.random.binomial(1, 0.2):\n",
    "#                 ke = augment_energy(ke, den)\n",
    "#             if np.random.binomial(1, 0.2):\n",
    "#                 ke = augment_energy(ke, den)\n",
    "            \n",
    "            vdx = F.pad((vel[:, 1, :, 1:] - vel[:, 1, :, :-1])[:, None], (0, 1, 0, 0), mode='reflect')\n",
    "            udy = F.pad((vel[:, 0, 1:] - vel[:, 0, :-1])[:, None], (0, 0, 0, 1), mode='reflect')\n",
    "\n",
    "            #w = udx - vdy\n",
    "            w = vdx - udy\n",
    "             \n",
    "            ke_mask = torch.tensor(np.random.binomial(1, 0.5, size=(BATCH_SIZE, 1, 1, 1)), device=device)\n",
    "            w_mask = torch.tensor(np.random.binomial(1, 0.5, size=(BATCH_SIZE, 1, 1, 1)), device=device)\n",
    "            s_mask = torch.tensor(np.random.binomial(1, 0.5, size=(BATCH_SIZE, 1)), device=device)\n",
    "            \n",
    "            ke = ke * (1.0 - ke_mask) - ke_mask * torch.ones_like(ke)\n",
    "            w = w * (1.0 - w_mask) - w_mask * torch.ones_like(w)\n",
    "            s = s * (1.0 - s_mask) - s_mask * torch.ones_like(s)\n",
    "            \n",
    "            loss = train_step(G, D, den, vel, s, w, ke, optG, optD, BATCH_SIZE)\n",
    "            \n",
    "            for k in loss.keys():\n",
    "                losses[k][-1].append(loss[k])\n",
    "        for k in losses.keys():\n",
    "            losses[k][-1] = np.mean(losses[k][-1])\n",
    "        \n",
    "        print(f'Epoch:{ epoch}')\n",
    "        \n",
    "        for k in loss.keys():\n",
    "            print(f'{k}: {losses[k][-1]}')\n",
    "        \n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            for den, vel, s in tqdm(val_loader):\n",
    "                den, vel, s = den.to(device), vel.to(device), s.to(device),\n",
    "                g_out, *_ = G(den)\n",
    "                u = curl(g_out).cpu().numpy()\n",
    "                if np.random.binomial(1, p=0.05):\n",
    "                    plt.imshow(u.pow(2).sum(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "75a88b9f",
   "metadata": {
    "cellId": "q5qydv81g7kf2ebn35dn3r"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "07479068",
   "metadata": {
    "cellId": "y590ywaalwis6tcwujc25j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G and D initialized\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "G_S_net = SubNetS(16)\n",
    "G_W_net = SubNetW(145)\n",
    "G_KE_net = SubNetKE(16)\n",
    "\n",
    "G = UNet(1, 1, G_S_net, G_W_net, G_KE_net)\n",
    "\n",
    "D_S_net = SubNetS(16)\n",
    "D_W_net = SubNetW(145)\n",
    "D_KE_net = SubNetKE(16)\n",
    "\n",
    "D = UNet(2, 1, D_S_net, D_W_net, D_KE_net)\n",
    "\n",
    "G.to(device), D.to(device)\n",
    "print('G and D initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "790b8d7e",
   "metadata": {
    "cellId": "7gb4niwqo1d9icf0y2d8wh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "optG = torch.optim.Adam(G.parameters(), lr=2e-4)\n",
    "optD = torch.optim.Adam(D.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6a5fd",
   "metadata": {
    "cellId": "pt89nisblgdcyuc7wfauh8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a66c7bd7cf421eb22a7ec07792d96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=22800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g1.1\n",
    "train(G, D, optG, optD, train_loader, val_loader, 100, device, BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "7eafd645-2046-4633-b926-27282db9841c",
  "notebookPath": "den2vel.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
